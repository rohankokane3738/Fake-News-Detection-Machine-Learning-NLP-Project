{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake News Classification with The Help Of Natural Language Processing Technique.\n",
    "Fake news detection is a hot topic in the field of natural language processing. We consume news through several mediums throughout the day in our daily routine, but sometimes it becomes difficult to decide which one is fake and which one is authentic. Our job is to create a model which predicts whether a given news is real or fake.\n",
    "\n",
    "Unsupported Cell Type. Double-Click to inspect/edit the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"News_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna() #Handled Missing values by droping those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['id','text','author'],axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_data = 'The quick brown fox jumps over the lazy dog'\n",
    "sample_data = sample_data.split()\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_data = [data.lower() for data in sample_data]\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Print the first 10 stopwords and the total number of stopwords\n",
    "print(stopwords_list[:10])\n",
    "print(len(stopwords_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Assuming sample_data is a list of words you want to filter\n",
    "sample_data = [\"dogs\", \"running\", \"better\", \"is\", \"a\", \"the\", \"in\"]\n",
    "\n",
    "# Filter out stopwords\n",
    "filtered_data = [data for data in sample_data if data not in stopwords_list]\n",
    "\n",
    "print(filtered_data)\n",
    "print(len(filtered_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Assuming sample_data is a list of words\n",
    "sample_data_stemming = [ps.stem(data) for data in sample_data]\n",
    "\n",
    "print(sample_data_stemming)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "# Assuming sample_data is a list of words you want to lemmatize\n",
    "sample_data = [\"dogs\", \"running\", \"better\"]\n",
    "\n",
    "# Lemmatize the sample_data\n",
    "sample_data_lemma = [lm.lemmatize(data) for data in sample_data]\n",
    "\n",
    "# Print the lemmatized result\n",
    "print(sample_data_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(df)):\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', df['title'][i])  # Fix the regex\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lm.lemmatize(x) for x in review if x not in stopwords_list]  # Use stopwords_list\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Vectorization (Convert Text data into the Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "x = tf.fit_transform(corpus).toarray()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y = df['label']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splitting into the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 10, stratify = y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(x_train),len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(x_test)\n",
    "accuracy_score_ = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score on Test Data Set:\", accuracy_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, model, x_train, x_test, y_train, y_test):\n",
    "        self.model = model\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def train_evaluation(self):\n",
    "        y_pred_train = self.model.predict(self.x_train)\n",
    "        acc_scr_train = accuracy_score(self.y_train, y_pred_train)\n",
    "        print(\"Accuracy Score On Training Data Set:\", acc_scr_train)\n",
    "        print()\n",
    "        con_mat_train = confusion_matrix(self.y_train, y_pred_train)\n",
    "        print(\"Confusion Matrix On Training Data Set:\\n\", con_mat_train)\n",
    "        print()\n",
    "        class_rep_train = classification_report(self.y_train, y_pred_train)\n",
    "        print(\"Classification Report On Training Data Set:\\n\", class_rep_train)\n",
    "\n",
    "    def test_evaluation(self):\n",
    "        y_pred_test = self.model.predict(self.x_test)\n",
    "        acc_scr_test = accuracy_score(self.y_test, y_pred_test)\n",
    "        print(\"Accuracy Score On Testing Data Set:\", acc_scr_test)\n",
    "        print()\n",
    "        con_mat_test = confusion_matrix(self.y_test, y_pred_test)\n",
    "        print(\"Confusion Matrix On Testing Data Set:\\n\", con_mat_test)\n",
    "        print()\n",
    "        class_rep_test = classification_report(self.y_test, y_pred_test)\n",
    "        print(\"Classification Report On Testing Data Set:\\n\", class_rep_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "evaluation = Evaluation(rf, x_train, x_test, y_train, y_test)\n",
    "evaluation.train_evaluation()\n",
    "evaluation.test_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def text_preprocessing_user(self):\n",
    "        lm = WordNetLemmatizer()\n",
    "        pred_data = [self.data]\n",
    "        preprocess_data = []\n",
    "        stopwords_set = set(stopwords.words('english'))  # Use set for faster membership testing\n",
    "        for data in pred_data:\n",
    "            review = re.sub(r'[^a-zA-Z]', ' ', data)  # Fix the regex\n",
    "            review = review.lower()\n",
    "            review = review.split()\n",
    "            review = [lm.lemmatize(x) for x in review if x not in stopwords_set]  # Use stopwords_set\n",
    "            review = \" \".join(review)\n",
    "            preprocess_data.append(review)\n",
    "        return preprocess_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Checking the accuracy on testing dataset\n",
    "Evaluation(rf,x_train, x_test, y_train, y_test).test_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = 'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'\n",
    "preprocessed_data = Preprocessing(data).text_preprocessing_user()\n",
    "print(preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    def __init__(self, pred_data, model):\n",
    "        self.pred_data = pred_data\n",
    "        self.model = model\n",
    "\n",
    "    def prediction_model(self):\n",
    "        preprocess_data = Preprocessing(self.pred_data).text_preprocessing_user()\n",
    "        # Assuming you have defined tf somewhere in your code\n",
    "        data = tf.transform(preprocess_data)\n",
    "        prediction = self.model.predict(data)\n",
    "\n",
    "        if prediction[0] == 0:\n",
    "            return \"The News Is Fake\"\n",
    "        else:\n",
    "            return \"The News Is Real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "# Load the English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Your data\n",
    "data = 'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'\n",
    "\n",
    "# Text preprocessing\n",
    "review = data.lower()\n",
    "review = review.split()\n",
    "review = [lm.lemmatize(x) for x in review if x not in stopwords_list]\n",
    "review = \" \".join(review)\n",
    "\n",
    "print(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "\n",
    "    def __init__(self,pred_data, model):\n",
    "        self.pred_data = pred_data\n",
    "        self.model = model\n",
    "\n",
    "    def prediction_model(self):\n",
    "        preprocess_data = Preprocessing(self.pred_data).text_preprocessing_user()\n",
    "        data = tf.transform(preprocess_data)\n",
    "        prediction = self.model.predict(data)\n",
    "\n",
    "        if prediction [0] == 0 :\n",
    "            return \"The News Is Fake\"\n",
    "\n",
    "        else:\n",
    "            return \"The News Is Real\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = 'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'\n",
    "prediction_result = Prediction(data, rf).prediction_model()\n",
    "print(prediction_result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['title'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "user_data = '15 Civilians Killed In Single US Airstrike Have Been Identified'\n",
    "Prediction(user_data,rf).prediction_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
